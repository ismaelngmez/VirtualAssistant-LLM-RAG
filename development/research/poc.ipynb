{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.vectorstores import FAISS\n",
    "import os, yaml, time, mlflow, pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read config.yaml\n",
    "config_path = '/jupyterlab/config/config.yaml'\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "params = {\n",
    "    'main_model_id': 'GritLM-7B',\n",
    "    'main_model_temperature': 0.7,\n",
    "    'main_model_max_new_tokens': 200,\n",
    "    'embeddings_model_id': 'mxbai-embed-large-v1',\n",
    "    'normalize_embeddings': False,\n",
    "    'search_k': 1,\n",
    "    'chunk_size': 1024,\n",
    "    'chunk_overlap': 128,\n",
    "    'template': '''\n",
    "    DOCUMENT: {} \n",
    "    \n",
    "    QUESTION: {} \n",
    "    \n",
    "    INSTRUCTIONS: \n",
    "    Answer the users QUESTION using the DOCUMENT text above. Keep your answer ground in the facts of the DOCUMENT.\n",
    "    '''\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\")\n",
    "# 8-bit quantization\n",
    "# bnb_config = BitsAndBytesConfig(load_in_8bit=True, activation_dtype=\"int8\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('/jupyterlab' + params['main_model_id'], quantization_config=bnb_config, device_map = 'cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained('/jupyterlab/' + params['main_model_id'], device_map = 'cuda', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('/jupyterlab/' + params['main_model_id'], device_map = 'cuda', trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('/jupyterlab/' + params['main_model_id'], device_map = 'cuda', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pipeline generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer, max_new_tokens = params['main_model_max_new_tokens'])\n",
    "\n",
    "model_kwargs = {\n",
    "    'temperature': params['main_model_temperature'],\n",
    "    'device' : 0\n",
    "}\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline = pipe,\n",
    "    model_kwargs=model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_manual_path = config.get('new_manual_path')\n",
    "loader = UnstructuredMarkdownLoader(new_manual_path) \n",
    "document= loader.load()\n",
    "\n",
    "splitter = CharacterTextSplitter(chunk_size=params['chunk_size'], chunk_overlap=params['chunk_overlap'], separator='\\n')\n",
    "documents = splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {'device': 0}\n",
    "encode_kwargs = {'normalize_embeddings': params['normalize_embeddings']}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name='/jupyterlab/' + params['embeddings_model_id'],\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_path = '/jupyterlab/datasets/test_data.jsonl'\n",
    "df = pd.read_json(jsonl_path, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(question):\n",
    "    docs = db.similarity_search(question, topk=params['search_k'])\n",
    "    context = '\\n'.join([doc.page_content for doc in docs])\n",
    "\n",
    "    query = params['template'].format(context, question)\n",
    "\n",
    "    ini = time.time()\n",
    "    r = llm.invoke(query)\n",
    "    fin = time.time()\n",
    "    duration = fin - ini\n",
    "    r = r[len(query):]\n",
    "    \n",
    "    return r, docs, duration\n",
    "\n",
    "df[['Prediction', 'Sources', 'Time']] = df['Question'].apply(lambda q: pd.Series(query(q)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(row, evaluator):\n",
    "    question = row['Question']\n",
    "    answer = row['Answer']\n",
    "    prediction = row['Prediction']\n",
    "    return evaluator.evaluate_strings(input=question, prediction=prediction, reference=answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Embedding distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = load_evaluator(\"embedding_distance\", embeddings=embeddings)\n",
    "df['Embedding Distance'] = df.apply(lambda r: evaluation(r, evaluator=evaluator)['score'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### String distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = load_evaluator(\"string_distance\")\n",
    "df['String Distance'] = df.apply(lambda r: evaluation(r, evaluator=evaluator)['score'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Criteria Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = load_evaluator(\"labeled_criteria\", llm=llm, criteria=\"relevance\")\n",
    "\n",
    "df[['Criteria Reasoning', 'Criteria Value', 'Criteria Score']] = df.apply(lambda r: pd.Series(evaluation(r, evaluator=evaluator)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_embedding_dist = df['Embedding Distance'].sum()\n",
    "sum_string_dist = df['String Distance'].sum()\n",
    "sum_criteria_score = df['Criteria Score'].sum()\n",
    "sum_time = df['Time'].sum()\n",
    "\n",
    "df.loc[len(df)] = [\"\", \"\", \"\", \"\", sum_time, sum_embedding_dist, sum_string_dist, \"\", \"\", sum_criteria_score]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPUtil as GPU\n",
    "mem_gpu = GPU.getGPUs()[0].memoryUsed\n",
    "\n",
    "GPUs = GPU.getGPUs()\n",
    "for gpu in GPUs:\n",
    "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    print(row['Question'])\n",
    "    print(row['Answer'])\n",
    "    print(row['Prediction'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MLFlow server credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "mlflow_df = df[['Question', 'Answer', 'Prediction']]\n",
    "jsonl_filename = \"df.jsonl\"\n",
    "with jsonlines.open(jsonl_filename, mode='w') as writer:\n",
    "    for index, row in mlflow_df.iterrows():\n",
    "        writer.write(row.to_dict())\n",
    "\n",
    "experiment_name = 'Optimisation of hyperparameters'\n",
    "\n",
    "if not mlflow.get_experiment_by_name(experiment_name):\n",
    "    mlflow.create_experiment(name=experiment_name)\n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "with mlflow.start_run(experiment_id = experiment.experiment_id):\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metric('Total Embedding Distance', sum_embedding_dist)\n",
    "    mlflow.log_metric('Total String Distance', sum_string_dist)\n",
    "    mlflow.log_metric('Total Criteria Score', sum_criteria_score)\n",
    "    mlflow.log_metric('Total Time', sum_time)\n",
    "    mlflow.log_metric('Size in MB', mem_gpu)\n",
    "    mlflow.log_artifact(jsonl_filename, \"df.jsonl\")\n",
    "\n",
    "os.remove(jsonl_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cite sources\n",
    "\n",
    "import textwrap\n",
    "\n",
    "def wrap_text_preserve_newlines(text, width=110):\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
    "    print('\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df['Question'][0]\n",
    "\n",
    "print('-------------------Instructor Embeddings------------------\\n')\n",
    "print(query)\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
